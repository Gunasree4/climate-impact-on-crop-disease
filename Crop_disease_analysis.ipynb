{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeknWzyA24tNhZ4NJpIvOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gunasree4/climate-impact-on-crop-disease/blob/main/Crop_disease_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2DpYOhavLF1"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install (optional) and imports\n",
        "!pip install -q pandas matplotlib seaborn scikit-learn openpyxl\n",
        "\n",
        "import os, sys, math, zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             roc_auc_score, roc_curve, mean_squared_error, r2_score)\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "\n",
        "import joblib\n",
        "from IPython.display import HTML, display\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Helper functions for EDA, preprocessing and modelling\n",
        "\n",
        "RESULTS_DIR = \"/content/project_results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "def save_fig(fig, name):\n",
        "    path = os.path.join(RESULTS_DIR, name)\n",
        "    fig.savefig(path, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    return path\n",
        "\n",
        "def quick_stats(df):\n",
        "    stats = df.describe(include='all').transpose()\n",
        "    stats.to_csv(os.path.join(RESULTS_DIR, \"data_summary.csv\"))\n",
        "    return stats\n",
        "\n",
        "def detect_target(df):\n",
        "    # Heuristic: common names first, else last column\n",
        "    common_names = ['target','label','y','outcome','class','disease_risk','Disease_Risk']\n",
        "    for name in common_names:\n",
        "        if name in df.columns:\n",
        "            return name\n",
        "    # if dataset has a boolean/int column with only 0/1, prefer it\n",
        "    for col in df.columns[::-1]:\n",
        "        if df[col].dropna().isin([0,1]).all() and df[col].nunique()<=2:\n",
        "            return col\n",
        "    # else choose last column\n",
        "    return df.columns[-1]\n",
        "\n",
        "def task_type_from_target(series):\n",
        "    # classification if few unique values or dtype is object/categorical\n",
        "    if series.dtype == 'object' or series.dtype.name.startswith('category'):\n",
        "        return 'classification'\n",
        "    nunique = series.dropna().nunique()\n",
        "    if nunique <= 20:\n",
        "        return 'classification'\n",
        "    return 'regression'\n",
        "\n",
        "def plot_missing(df):\n",
        "    miss = df.isnull().mean().sort_values(ascending=False)\n",
        "    fig, ax = plt.subplots(figsize=(8, max(3, min(12, len(miss)/2))))\n",
        "    miss.plot.bar(ax=ax)\n",
        "    ax.set_ylabel(\"Fraction missing\")\n",
        "    ax.set_title(\"Missing values by column\")\n",
        "    path = save_fig(fig, \"missing_values.png\")\n",
        "    return path\n",
        "\n",
        "def plot_corr(df, numeric_cols):\n",
        "    if len(numeric_cols) < 2:\n",
        "        return None\n",
        "    fig, ax = plt.subplots(figsize=(8,6))\n",
        "    sns.heatmap(df[numeric_cols].corr(), annot=True, fmt=\".2f\", ax=ax, cmap=\"YlGnBu\")\n",
        "    ax.set_title(\"Correlation matrix (numeric)\")\n",
        "    path = save_fig(fig, \"correlation_matrix.png\")\n",
        "    return path\n",
        "\n",
        "def value_counts_plots(df):\n",
        "    cats = df.select_dtypes(include=['object','category']).columns.tolist()\n",
        "    paths = []\n",
        "    for c in cats:\n",
        "        fig, ax = plt.subplots(figsize=(6,4))\n",
        "        vc = df[c].value_counts().nlargest(20)\n",
        "        vc.plot.bar(ax=ax)\n",
        "        ax.set_title(f\"Value counts: {c}\")\n",
        "        paths.append(save_fig(fig, f\"vc_{c}.png\"))\n",
        "    return paths\n",
        "\n",
        "def feature_importance_plot(model, feature_names, prefix=\"feature_importance\"):\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        imp = model.feature_importances_\n",
        "    elif hasattr(model, \"coef_\"):\n",
        "        imp = np.abs(model.coef_).ravel()\n",
        "    else:\n",
        "        return None\n",
        "    idx = np.argsort(imp)[::-1][:30]\n",
        "    names = [feature_names[i] for i in idx]\n",
        "    vals = imp[idx]\n",
        "    fig, ax = plt.subplots(figsize=(8, min(12, len(names)/1.5)))\n",
        "    sns.barplot(x=vals, y=names, ax=ax)\n",
        "    ax.set_title(\"Feature importance\")\n",
        "    path = save_fig(fig, f\"{prefix}.png\")\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "hBEt8lFgv9C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Main automation function that accepts a dataframe and (optionally) target column\n",
        "def run_automation(df, target_col=None, save_results=True, test_size=0.2, random_state=42):\n",
        "    report = {}\n",
        "    df0 = df.copy()\n",
        "    stats = quick_stats(df0)\n",
        "    report['summary_path'] = os.path.join(RESULTS_DIR, \"data_summary.csv\")\n",
        "\n",
        "    # detect target if not provided\n",
        "    if target_col is None:\n",
        "        target_col = detect_target(df0)\n",
        "    report['target_col'] = target_col\n",
        "    y = df0[target_col]\n",
        "    X = df0.drop(columns=[target_col])\n",
        "\n",
        "    task = task_type_from_target(y)\n",
        "    report['task'] = task\n",
        "\n",
        "    # Basic EDA plots\n",
        "    report['missing_plot'] = plot_missing(df0)\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    report['corr_plot'] = plot_corr(df0, numeric_cols)\n",
        "    report['vc_plots'] = value_counts_plots(df0)\n",
        "\n",
        "    # Preprocessing: simple imputer + encoding + scaling\n",
        "    num_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "    cat_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "    ])\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', num_transformer, numeric_cols),\n",
        "        ('cat', cat_transformer, cat_cols)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    # Choose models\n",
        "    models = {}\n",
        "    if task == 'classification':\n",
        "        models = {\n",
        "            'LogisticRegression': LogisticRegression(max_iter=500),\n",
        "            'RandomForest': RandomForestClassifier(n_estimators=200, random_state=random_state),\n",
        "            'GradientBoosting': GradientBoostingClassifier(n_estimators=150, random_state=random_state)\n",
        "        }\n",
        "        scoring = 'accuracy'\n",
        "    else:\n",
        "        models = {\n",
        "            'LinearRegression': LinearRegression(),\n",
        "            'RandomForest': RandomForestRegressor(n_estimators=200, random_state=random_state),\n",
        "            'GradientBoosting': GradientBoostingRegressor(n_estimators=150, random_state=random_state)\n",
        "        }\n",
        "        scoring = 'neg_root_mean_squared_error'\n",
        "\n",
        "    # Split\n",
        "    if task == 'classification':\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
        "                                                            random_state=random_state, stratify=y.fillna(method='ffill'))\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "    results = {}\n",
        "    feature_names_after_preproc = None\n",
        "\n",
        "    for name, model in models.items():\n",
        "        pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
        "        # cross-validation\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state) if task=='classification' else KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "        try:\n",
        "            scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "            mean_score = scores.mean()\n",
        "        except Exception as e:\n",
        "            mean_score = None\n",
        "        # fit on full train set\n",
        "        pipe.fit(X_train, y_train)\n",
        "        preds = pipe.predict(X_test)\n",
        "        # metrics\n",
        "        if task == 'classification':\n",
        "            acc = accuracy_score(y_test, preds)\n",
        "            try:\n",
        "                probas = pipe.predict_proba(X_test)[:,1]\n",
        "                auc = roc_auc_score(y_test, probas)\n",
        "            except:\n",
        "                auc = None\n",
        "            creport = classification_report(y_test, preds, output_dict=True)\n",
        "            results[name] = {'cv_score': mean_score, 'accuracy': acc, 'auc': auc, 'report': creport}\n",
        "        else:\n",
        "            rmse = mean_squared_error(y_test, preds, squared=False)\n",
        "            r2 = r2_score(y_test, preds)\n",
        "            results[name] = {'cv_score': mean_score, 'rmse': rmse, 'r2': r2}\n",
        "\n",
        "        # feature names extraction (after preprocessor)\n",
        "        # get feature names if cat onehot used\n",
        "        try:\n",
        "            preproc = pipe.named_steps['preprocessor']\n",
        "            num_cols = numeric_cols\n",
        "            cat_ohe = []\n",
        "            if len(cat_cols)>0:\n",
        "                ohe = preproc.named_transformers_['cat'].named_steps['onehot']\n",
        "                ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
        "                cat_ohe = list(ohe_cols)\n",
        "            feature_names_after_preproc = list(num_cols) + cat_ohe\n",
        "            # feature importance if available\n",
        "            model_obj = pipe.named_steps['model']\n",
        "            fi_path = feature_importance_plot(model_obj, feature_names_after_preproc, prefix=f\"fi_{name}\")\n",
        "            if fi_path:\n",
        "                results[name]['feature_importance_plot'] = fi_path\n",
        "        except Exception as e:\n",
        "            # ignore\n",
        "            pass\n",
        "\n",
        "        # save model\n",
        "        joblib.dump(pipe, os.path.join(RESULTS_DIR, f\"model_{name}.joblib\"))\n",
        "\n",
        "    # choose best\n",
        "    if task == 'classification':\n",
        "        best = max(results.items(), key=lambda kv: kv[1].get('accuracy', -999))\n",
        "    else:\n",
        "        # minimize rmse\n",
        "        best = min(results.items(), key=lambda kv: kv[1].get('rmse', float('inf')))\n",
        "    report['results'] = results\n",
        "    report['best_model'] = best[0]\n",
        "    # save results summary\n",
        "    pd.Series(report).to_json(os.path.join(RESULTS_DIR, \"run_report.json\"))\n",
        "\n",
        "    # write a human-readable conclusion\n",
        "    with open(os.path.join(RESULTS_DIR, \"conclusion.txt\"), \"w\") as f:\n",
        "        f.write(\"AUTO ANALYSIS SUMMARY\\n\")\n",
        "        f.write(f\"Detected target column: {target_col}\\n\")\n",
        "        f.write(f\"Detected task type: {task}\\n\")\n",
        "        f.write(f\"Models evaluated: {', '.join(results.keys())}\\n\")\n",
        "        f.write(f\"Best model: {best[0]}\\n\\n\")\n",
        "        f.write(\"Model metrics summary:\\n\")\n",
        "        for m, r in results.items():\n",
        "            f.write(f\"\\n--- {m} ---\\n\")\n",
        "            for k,v in r.items():\n",
        "                f.write(f\"{k}: {v}\\n\")\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "_kPQMVwzwAPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Upload or load file\n",
        "from google.colab import files\n",
        "print(\"Option A: upload file from your computer (small files).\")\n",
        "print(\"Option B: mount Google Drive and give path (for larger files).\")\n",
        "\n",
        "# Try to upload\n",
        "uploaded = files.upload()\n",
        "if len(uploaded) == 0:\n",
        "    # fallback: ask user to mount drive manually\n",
        "    print(\"No files uploaded â€” please mount Google Drive and set file path manually.\")\n",
        "else:\n",
        "    # take the first uploaded file\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    print(\"Uploaded:\", fname)\n",
        "    ext = fname.split('.')[-1].lower()\n",
        "    if ext in ['csv','txt']:\n",
        "        df = pd.read_csv(fname)\n",
        "    elif ext in ['xlsx','xls']:\n",
        "        df = pd.read_excel(fname)\n",
        "    else:\n",
        "        # try CSV read\n",
        "        try:\n",
        "            df = pd.read_csv(fname)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Could not read the uploaded file. Please upload a CSV/XLSX.\")\n",
        "    print(\"Data loaded. Shape:\", df.shape)\n",
        "    display(df.head())\n"
      ],
      "metadata": {
        "id": "Cck0bIuqwHu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVNSDETaDt-b",
        "outputId": "71c36fbb-d6a2-4f73-8bed-e72bab7ce83c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Run automation on the loaded dataframe\n",
        "# If you want to force a target column, set target_col = \"YourColumnName\"\n",
        "target_col = None  # <-- change if you want to force target column name (string)\n",
        "report = run_automation(df, target_col=target_col)\n",
        "print(\"Automation finished. Results saved to:\", RESULTS_DIR)\n",
        "print(\"Detected task:\", report['task'])\n",
        "print(\"Detected target column:\", report['target_col'])\n",
        "print(\"Best model:\", report['best_model'])\n"
      ],
      "metadata": {
        "id": "dehiQgdOwJ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: show main files and create zip for download\n",
        "!ls -lah /content/project_results || true\n",
        "\n",
        "# Create a zip for easy download\n",
        "zipf = \"/content/project_results.zip\"\n",
        "with zipfile.ZipFile(zipf, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "    for root, dirs, files in os.walk(RESULTS_DIR):\n",
        "        for file in files:\n",
        "            z.write(os.path.join(root, file), arcname=os.path.join(os.path.relpath(root, RESULTS_DIR), file))\n",
        "\n",
        "print(\"Results zipped:\", zipf)\n",
        "from google.colab import files\n",
        "files.download(zipf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "xRj-GET0wM4O",
        "outputId": "d1520a4e-21a9-4023-dfd9-0ab59bb8819a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8.0K\n",
            "drwxr-xr-x 2 root root 4.0K Nov  7 05:28 .\n",
            "drwxr-xr-x 1 root root 4.0K Nov  7 05:30 ..\n",
            "Results zipped: /content/project_results.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b17a8703-f04c-4588-9225-cb9c9883ed21\", \"project_results.zip\", 22)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}